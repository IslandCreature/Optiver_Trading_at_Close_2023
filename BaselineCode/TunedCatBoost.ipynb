{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T15:19:36.124268Z","iopub.status.busy":"2023-12-17T15:19:36.123931Z","iopub.status.idle":"2023-12-17T15:19:37.105479Z","shell.execute_reply":"2023-12-17T15:19:37.104328Z","shell.execute_reply.started":"2023-12-17T15:19:36.124239Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import gc\n","import time\n","import warnings\n","from warnings import simplefilter\n","from itertools import combinations\n","\n","import joblib\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm\n","from sklearn.metrics import mean_absolute_error\n","\n","warnings.filterwarnings(\"ignore\")\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T15:19:37.107837Z","iopub.status.busy":"2023-12-17T15:19:37.107330Z","iopub.status.idle":"2023-12-17T15:19:38.398483Z","shell.execute_reply":"2023-12-17T15:19:38.397344Z","shell.execute_reply.started":"2023-12-17T15:19:37.107809Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Config option `kernel_spec_manager_class` not recognized by `EnableNBExtensionApp`.\n","Enabling notebook extension jupyter-js-widgets/extension...\n","      - Validating: \u001b[32mOK\u001b[0m\n"]}],"source":["!jupyter nbextension enable --py widgetsnbextension"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T15:19:38.400248Z","iopub.status.busy":"2023-12-17T15:19:38.399927Z","iopub.status.idle":"2023-12-17T15:19:38.410795Z","shell.execute_reply":"2023-12-17T15:19:38.409826Z","shell.execute_reply.started":"2023-12-17T15:19:38.400216Z"},"trusted":true},"outputs":[],"source":["# Set up parameters\n","is_offline = False    # Flag for online/offline mode\n","is_train = True    # Flag for training mode\n","is_infer = True    # Flag for inference mode\n","split_day = 435    # Split day for time series data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T15:19:38.413467Z","iopub.status.busy":"2023-12-17T15:19:38.413183Z","iopub.status.idle":"2023-12-17T15:19:51.916387Z","shell.execute_reply":"2023-12-17T15:19:51.915413Z","shell.execute_reply.started":"2023-12-17T15:19:38.413443Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(5237892, 17)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n","df = df.dropna(subset=[\"target\"])\n","df.reset_index(drop=True, inplace=True)\n","df.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-12-17T15:19:51.917976Z","iopub.status.busy":"2023-12-17T15:19:51.917686Z","iopub.status.idle":"2023-12-17T15:19:51.933813Z","shell.execute_reply":"2023-12-17T15:19:51.932960Z","shell.execute_reply.started":"2023-12-17T15:19:51.917951Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df, verbose=0):\n","    \"\"\"\n","    Iterate through all numeric columns of a dataframe and modify the data type\n","    to reduce memory usage.\n","    \"\"\"\n","    # Calculate the initial memory usage of the DataFrame\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            \n","            if str(col_type)[:3] == \"int\":\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                # Check if the column's data type is a float\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float32)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float32)\n","                    \n","    if verbose:\n","        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n","        end_mem = df.memory_usage().sum() / 1024**2\n","        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n","        decrease = 100 * (start_mem - end_mem) / start_mem\n","        logger.info(f\"Decreased by {decrease:.2f}%\")\n","\n","    return df"]},{"cell_type":"code","execution_count":6,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-12-17T15:19:51.935677Z","iopub.status.busy":"2023-12-17T15:19:51.935077Z","iopub.status.idle":"2023-12-17T15:19:52.131273Z","shell.execute_reply":"2023-12-17T15:19:52.130525Z","shell.execute_reply.started":"2023-12-17T15:19:51.935642Z"},"trusted":true},"outputs":[],"source":["# Import Numba for just-in-time (JIT) compilation and parallel processing\n","from numba import njit, prange\n","\n","# Function to compute triplet imbalance in parallel using Numba\n","@njit(parallel=True)\n","def compute_triplet_imbalance(df_values, comb_indices):\n","    num_rows = df_values.shape[0]\n","    num_combinations = len(comb_indices)\n","    imbalance_features = np.empty((num_rows, num_combinations))\n","\n","    # Loop through all combinations of triplets\n","    for i in prange(num_combinations):\n","        a, b, c = comb_indices[i]\n","        \n","        # Loop through rows of the DataFrame\n","        for j in range(num_rows):\n","            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n","            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n","            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n","            \n","            # Prevent division by zero\n","            if mid_val == min_val:\n","                imbalance_features[j, i] = np.nan\n","            else:\n","                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n","\n","    return imbalance_features\n","\n","\n","# Function to calculate triplet imbalance for given price data and a DataFrame\n","def calculate_triplet_imbalance_numba(price, df):\n","    # Convert DataFrame to numpy array for Numba compatibility\n","    df_values = df[price].values\n","    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n","\n","    # Calculate the triplet imbalance using the Numba-optimized function\n","    features_array = compute_triplet_imbalance(df_values, comb_indices)\n","\n","    # Create a DataFrame from the results\n","    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n","    features = pd.DataFrame(features_array, columns=columns)\n","\n","    return features"]},{"cell_type":"code","execution_count":7,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-12-17T15:19:52.133297Z","iopub.status.busy":"2023-12-17T15:19:52.132782Z","iopub.status.idle":"2023-12-17T15:19:52.152800Z","shell.execute_reply":"2023-12-17T15:19:52.151892Z","shell.execute_reply.started":"2023-12-17T15:19:52.133261Z"},"trusted":true},"outputs":[],"source":["# Function to generate imbalance features\n","def imbalance_features(df):\n","    # Define lists of price and size-related column names\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","\n","    # V1 features\n","    # Calculate various features using Pandas eval function\n","    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n","    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n","    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n","    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n","    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n","    \n","    # Create features for pairwise price imbalances\n","    for c in combinations(prices, 2):\n","        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n","        \n","    # V2 features\n","    # Calculate additional features\n","    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n","    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n","    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n","    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n","    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n","    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n","    \n","    # V3 features\n","    # Calculate shifted and return features for specific columns\n","    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n","        for window in [1, 2, 3, 10]:\n","            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n","            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n","    \n","    # Calculate diff features for specific columns\n","    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n","                'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n","        for window in [1, 2, 3, 10]:\n","            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n","    # Replace infinite values with 0\n","    return df.replace([np.inf, -np.inf], 0)\n","\n","\n","def numba_imb_features(df):\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","    \n","    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n","        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n","        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n","        \n","    # Calculate triplet imbalance features using the Numba-optimized function\n","    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n","        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n","        df[triplet_feature.columns] = triplet_feature.values\n","    return df\n","\n","\n","# Function to generate time and stock-related features\n","def other_features(df):\n","    df[\"dow\"] = df[\"date_id\"] % 5                 # Day of the week\n","    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n","    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n","\n","    # Map global features to the DataFrame\n","    for key, value in global_stock_id_feats.items():\n","        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n","\n","    return df\n","\n","\n","# Function to generate all features by combining imbalance and other features\n","def generate_all_features(df):\n","    # Select relevant columns for feature generation\n","    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n","    df = df[cols]\n","    \n","    # Generate imbalance features\n","    df = imbalance_features(df)\n","    df = numba_imb_features(df)\n","    # Generate time and stock-related features\n","    df = other_features(df)\n","    gc.collect()\n","    \n","    # Select and return the generated features\n","    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n","    \n","    return df[feature_name]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T15:19:52.154393Z","iopub.status.busy":"2023-12-17T15:19:52.154064Z","iopub.status.idle":"2023-12-17T15:19:52.276014Z","shell.execute_reply":"2023-12-17T15:19:52.275019Z","shell.execute_reply.started":"2023-12-17T15:19:52.154361Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Online mode\n"]},{"data":{"text/plain":["0"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Check if the code is running in offline or online mode\n","if is_offline:\n","    # In offline mode, split the data into training and validation sets based on the split_day\n","    df_train = df[df[\"date_id\"] <= split_day]\n","    df_valid = df[df[\"date_id\"] > split_day]\n","    print(\"Offline mode\")\n","    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n","else:\n","    # In online mode, use the entire dataset for training\n","    df_train = df\n","    print(\"Online mode\")\n","    \n","del df\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-17T15:19:52.277889Z","iopub.status.busy":"2023-12-17T15:19:52.277441Z"},"trusted":true},"outputs":[],"source":["%%time\n","if is_train:\n","    global_stock_id_feats = {\n","        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n","        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n","        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n","        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n","        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n","        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n","    }\n","    if is_offline:\n","        df_train_feats = generate_all_features(df_train)\n","        print(\"Build Train Feats Finished.\")\n","        df_valid_feats = generate_all_features(df_valid)\n","        print(\"Build Valid Feats Finished.\")\n","        df_valid_feats = reduce_mem_usage(df_valid_feats)\n","    else:\n","        df_train_feats = generate_all_features(df_train)\n","        print(\"Build Online Train Feats Finished.\")\n","\n","    df_train_feats = reduce_mem_usage(df_train_feats)\n","    \n","feature_name = list(df_train_feats.columns)\n","print(f\"Feature length = {len(feature_name)}\")"]},{"cell_type":"markdown","metadata":{},"source":["CatBoost Feature Selection Tutorial can be found [here](https://github.com/catboost/tutorials/blob/master/feature_selection/select_features_tutorial.ipynb).\n","\n","It contains examples, parameters description and valuable explanations."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","# Train procedure\n","if is_train:\n","    offline_split = df_train['date_id']>(split_day - 45)\n","    df_offline_train = df_train_feats[~offline_split]\n","    df_offline_valid = df_train_feats[offline_split]\n","    df_offline_train_target = df_train['target'][~offline_split]\n","    df_offline_valid_target = df_train['target'][offline_split]\n","    df_train_target = df_train[\"target\"]\n","    del df_train\n","    gc.collect()\n","    \n","    ctb_params = dict(iterations=1200,\n","                      learning_rate=1.0,\n","                      depth=8,\n","                      l2_leaf_reg=30,\n","                      bootstrap_type='Bernoulli',\n","                      subsample=0.66,\n","                      loss_function='MAE',\n","                      eval_metric = 'MAE',\n","                      metric_period=100,\n","                      od_type='Iter',\n","                      od_wait=30,\n","                      task_type='GPU',\n","                      allow_writing_files=False,\n","                      )\n","    \n","    print(\"Feature Elimination Performing.\")\n","    ctb_model = CatBoostRegressor(**ctb_params)\n","    summary = ctb_model.select_features(\n","        df_offline_train[feature_name], df_offline_train_target,\n","        eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n","        features_for_select=feature_name,\n","        num_features_to_select=len(feature_name)-24,    # Dropping from 124 to 100\n","        steps=3,\n","        algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n","        shap_calc_type=EShapCalcType.Regular,\n","        train_final_model=False,\n","        plot=True,\n","    )\n","    \n","    print(\"Valid Model Training on Selected Features Subset.\")\n","    ctb_model = CatBoostRegressor(**ctb_params)\n","    ctb_model.fit(\n","        df_offline_train[summary['selected_features_names']], df_offline_train_target,\n","        eval_set=[(df_offline_valid[summary['selected_features_names']], df_offline_valid_target)],\n","        use_best_model=True,\n","    )\n","    \n","    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target\n","    gc.collect()\n","    \n","    print(\"Infer Model Training on Selected Features Subset.\")\n","    infer_params = ctb_params.copy()\n","    # CatBoost train best with Valid number of iterations\n","    infer_params[\"iterations\"] = ctb_model.best_iteration_\n","    infer_ctb_model = CatBoostRegressor(**infer_params)\n","    infer_ctb_model.fit(df_train_feats[summary['selected_features_names']], df_train_target)\n","    print(\"Infer Model Training on Selected Features Subset Complete.\")\n","    \n","    if is_offline:   \n","        # Offline predictions\n","        df_valid_target = df_valid[\"target\"]\n","        offline_predictions = infer_ctb_model.predict(df_valid_feats[summary['selected_features_names']])\n","        offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n","        print(f\"Offline Score {np.round(offline_score, 4)}\")\n","        del df_valid, df_valid_feats\n","        gc.collect()\n","    \n","    del df_train_feats\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["summary['eliminated_features_names']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feat_importances = infer_ctb_model.get_feature_importance(prettified=True)\n","\n","plt.figure(figsize=(12, 20))\n","sns.barplot(x=\"Importances\", y=\"Feature Id\", data=feat_importances)\n","plt.title('CatBoost features importance:')\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from catboost import EFstrType\n","feat_interactions = infer_ctb_model.get_feature_importance(type=EFstrType.Interaction, prettified=True)\n","top_interactions = feat_interactions[:10]\n","top_interactions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["top_interactions['First Feature Index'] = top_interactions['First Feature Index'].apply(lambda x: summary['selected_features_names'][x])\n","top_interactions['Second Feature Index'] = top_interactions['Second Feature Index'].apply(lambda x: summary['selected_features_names'][x])\n","top_interactions.columns = ['First Feature', 'Second Feature', 'Interaction']\n","top_interactions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","# Infer procedure\n","def zero_sum(prices, volumes):\n","    std_error = np.sqrt(volumes)\n","    step = np.sum(prices)/np.sum(std_error)\n","    out = prices-std_error*step\n","    return out\n","\n","if is_infer:\n","    import optiver2023\n","    env = optiver2023.make_env()\n","    iter_test = env.iter_test()\n","    counter = 0\n","    y_min, y_max = -64, 64\n","    qps, predictions = [], []\n","    cache = pd.DataFrame()\n","    \n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","        now_time = time.time()\n","        test = test.drop('currently_scored', axis=1)\n","        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n","        if counter > 0:\n","            cache = cache.groupby('stock_id').tail(21).reset_index(drop=True)\n","        feat = generate_all_features(cache)[-len(test):]\n","        \n","        # Selected features subset\n","        feat = feat[summary['selected_features_names']]\n","        \n","        ctb_predictions = infer_ctb_model.predict(feat)\n","        ctb_predictions = zero_sum(ctb_predictions, test['bid_size'] + test['ask_size'])\n","        clipped_predictions = np.clip(ctb_predictions, y_min, y_max)\n","        sample_prediction['target'] = clipped_predictions\n","        env.predict(sample_prediction)\n","        counter += 1\n","        qps.append(time.time() - now_time)\n","        if counter % 10 == 0:\n","            print(counter, 'qps:', np.mean(qps))\n","           \n","    time_cost = 1.146 * np.mean(qps)\n","    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7056235,"sourceId":57891,"sourceType":"competition"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
